{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16129f9c",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "In this notebook, we apply three clustering algorithms (DBSCAN, Agglomerative Clustering, and K-Means) to group patient data and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cfc1c",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ed1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")  # We load the preprocessed dataset\n",
    "\n",
    "# Drop any unnecessary columns, especially the class label 'Disease'\n",
    "df = df.drop(columns=[col for col in ['nan', 'Disease'] if col in df.columns], errors='ignore')\n",
    "\n",
    "# Standardize the data to normalize scale\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Apply DBSCAN algorithm with pre-defined parameters\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=15)\n",
    "labels = dbscan.fit_predict(df_scaled)\n",
    "\n",
    "# Store the cluster labels\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Calculate the number of clusters and noise points\n",
    "num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "noise = list(labels).count(-1)\n",
    "\n",
    "# Evaluate clustering using Silhouette Score\n",
    "silhouette = silhouette_score(df_scaled, labels) if num_clusters > 1 else \"N/A\"\n",
    "\n",
    "print(f\"Number of clusters: {num_clusters}\")\n",
    "print(f\"Noise points: {noise}\")\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(df['Cluster'].value_counts())\n",
    "\n",
    "# Visualize the clustering result using PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_2d = pca.fit_transform(df_scaled)\n",
    "colors = ['gray' if label == -1 else f'C{label}' for label in labels]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], c=colors, s=50)\n",
    "plt.title('DBSCAN Clustering Visualization (eps=0.8, min_samples=15)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae0f84",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering (HAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Try different values for the number of clusters and find the best based on silhouette score\n",
    "best_score = -1\n",
    "best_k = 0\n",
    "best_labels = None\n",
    "\n",
    "for k in range(2, 8):\n",
    "    hac = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    labels = hac.fit_predict(df_scaled)\n",
    "    score = silhouette_score(df_scaled, labels)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "        best_labels = labels\n",
    "\n",
    "# Assign best clustering result to DataFrame\n",
    "df['Cluster'] = best_labels\n",
    "\n",
    "print(f\"Best number of clusters: {best_k}\")\n",
    "print(f\"Best Silhouette Score: {best_score:.4f}\")\n",
    "print(df['Cluster'].value_counts())\n",
    "\n",
    "# Visualize the HAC clustering result using PCA\n",
    "data_2d = pca.fit_transform(df_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], c=best_labels, cmap='Set1', s=50)\n",
    "plt.title(f'HAC Clustering Visualization (k={best_k})')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564c43f",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use the Elbow method to determine optimal k\n",
    "inertia = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Method result\n",
    "plt.plot(K_range, inertia, marker='o')\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.show()\n",
    "\n",
    "# Apply KMeans with the selected k\n",
    "k_optimal = 5  # Change this if elbow plot shows a better value\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# Evaluate clustering using silhouette score\n",
    "sil_score = silhouette_score(df_scaled, df['Cluster'])\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "print(df['Cluster'].value_counts())\n",
    "\n",
    "# Visualize the K-Means result\n",
    "data_2d = pca.fit_transform(df_scaled)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], c=df['Cluster'], cmap='Set2', s=50)\n",
    "plt.title(f'K-Means Clustering Visualization (k={k_optimal})')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc9186",
   "metadata": {},
   "source": [
    "## How Clustering Helps\n",
    "\n",
    "Clustering helps identify patterns or groupings in the data that could enhance recommendations. For example, patients within the same cluster might share symptoms, and could receive similar treatments or suggestions.\n",
    "\n",
    "If clustering doesn’t directly improve recommendations, it still provides insight into the data structure and variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903346f",
   "metadata": {},
   "source": [
    "##  Visualizations and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446e1d8",
   "metadata": {},
   "source": [
    "###  DBSCAN: PCA Visualization\n",
    "The DBSCAN algorithm groups points based on density. In the plot below, we observe several clusters as well as noise points (gray), which DBSCAN marks as outliers. This makes DBSCAN powerful for identifying noise in data.\n",
    "\n",
    "![DBSCAN PCA](image/Figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e66f6e",
   "metadata": {},
   "source": [
    "###  HAC: PCA Visualization\n",
    "Hierarchical Agglomerative Clustering (HAC) divides the data into **3 clear clusters** using Ward linkage. The PCA projection below shows compact, separated clusters.\n",
    "\n",
    "![HAC PCA](image/Figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981f8ce",
   "metadata": {},
   "source": [
    "###  DBSCAN: k-NN Distance Plot\n",
    "This plot helps determine the best `eps` parameter for DBSCAN. We chose `eps = 1.2` because the red dashed line intersects where the curve rises significantly — indicating the border between dense regions and noise.\n",
    "\n",
    "![k-NN Distance](image/Figure_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383dcba",
   "metadata": {},
   "source": [
    "###  HAC: Dendrogram\n",
    "The dendrogram below represents the hierarchical structure of the clusters. We used this to decide on cutting the tree at 3 clusters.\n",
    "\n",
    "![Dendrogram](image/Figure_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ede98d",
   "metadata": {},
   "source": [
    "###  Visual Comparison Between Algorithms\n",
    "The figure below compares clustering results from **DBSCAN**, **KMeans**, and **HAC** side-by-side using PCA. \n",
    "- DBSCAN handled noise well but formed many small clusters.\n",
    "- KMeans formed clear and balanced clusters with k=5.\n",
    "- HAC showed a clean hierarchy, especially with 3 main groups.\n",
    "\n",
    "![Clustering Comparison](image/Figure_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51843e7b",
   "metadata": {},
   "source": [
    "###  Final K-Means Clusters\n",
    "K-Means gave **5 well-separated clusters**, which suggests that this method provides a balanced and interpretable structure for the data.\n",
    "\n",
    "![Final KMeans](image/Kmeans_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1c105",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "\n",
    "In the third phase of the project,We applied three clustering algorithms: K-Means, DBSCAN, and HAC, and used PCA (Principal Component Analysis) to reduce dimensionality and visually represent the clusters in 2D.\n",
    "\n",
    "### K-Means Clustering\n",
    "- The optimal number of clusters was determined using the Elbow Method, where k = 5 was selected.\n",
    "- We used n_init=10 to ensure stable and consistent clustering.\n",
    "- Silhouette Score = 0.2058\n",
    "- The clusters were imbalanced, with most data points grouped into Cluster 1.\n",
    "- PCA visualization showed partial overlap between clusters.\n",
    "\n",
    "###  DBSCAN Clustering\n",
    "- Parameters used: eps = 0.8, min_samples = 15\n",
    "- The algorithm identified 43 clusters and 1836 noise points.\n",
    "- Silhouette Score = 0.3541\n",
    "- The visualization clearly showed dense clusters, while gray-colored points represented noise (`label = -1`).\n",
    "\n",
    "###  HAC (Hierarchical Agglomerative Clustering)\n",
    "- We tested multiple values of k and selected the best using the Silhouette Score.\n",
    "- The best number of clusters was k = 3\n",
    "- Silhouette Score = 0.3234\n",
    "- PCA-based visualization showed well-separated hierarchical clusters.\n",
    "\n",
    "###  Visual Tools Used\n",
    "- All plots were generated using matplotlib and PCA from sklearn.decomposition.\n",
    "- Each visualization was displayed ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66984f",
   "metadata": {},
   "source": [
    "## Conclusion and Comparison\n",
    "\n",
    "- **DBSCAN** detected arbitrary-shaped clusters and identified noise, but may not have formed distinct clusters in sparse data.\n",
    "- **HAC** gave the best silhouette score and produced structured clusters.\n",
    "- **K-Means** was simple and efficient, with reasonable performance based on the elbow method.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

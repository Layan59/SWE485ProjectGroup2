# SWE485Project

My Role in the Project


üîπ Phase 1: Problem Understanding and Data Exploration

[BODOR]
 ‚Ä¢ Collected and examined a relevant dataset aligned with our project‚Äôs goal.
 ‚Ä¢ Provided samples of the raw dataset within the Jupyter Notebook for transparency.
 ‚Ä¢ Created visualizations such as histograms and tables to show:
 ‚Ä¢ Distribution of variables
 ‚Ä¢ Missing values
 ‚Ä¢ Statistical summaries (mean, variance, etc.)
 ‚Ä¢ Helped with initial preprocessing, including identifying and handling missing data.
 ‚Ä¢ Contributed to writing the dataset summary and description section.

[WAAD]
 ‚Ä¢ Handled the initial preprocessing of the dataset using Pandas and NumPy.
 ‚Ä¢ Applied data cleaning techniques such as:
 ‚Ä¢ Removing columns with only NaN values using dropna(axis=1, how='all')
 ‚Ä¢ Dropping irrelevant features (e.g., ‚ÄòDisease‚Äô column before clustering)
 ‚Ä¢ Checking for duplicate rows and ensuring data integrity
 ‚Ä¢ Performed basic data formatting and ensured columns were properly structured.
 ‚Ä¢ Verified dataset readiness for modeling by inspecting data types and null distributions.

[HUTOON]
I worked on describing the goal of the dataset and its source. This included explaining the purpose of the data and where it was collected from.

[Abrar]
‚Ä¢ Handled the preprocessing techniques for the selected dataset.
‚Ä¢ Applied data cleaning steps

[LAYAN]
‚Ä¢ Created the GitHub repository for the project and organized its folder structure to ensure clear collaboration.
‚Ä¢ Added the main dataset folder and uploaded the cleaned version used by all team members.
‚Ä¢ Wrote the introductory section in the Jupyter Notebook that includes:
- The project motivation
- Students‚Äô names
- A general overview of the project and its goal

üîπ Phase 2: Supervised Learning

[BODOR]
 ‚Ä¢ Created a Jupyter Notebook in the ‚ÄúSupervised Learning‚Äù folder.
 ‚Ä¢ Implemented multiple supervised learning algorithms (e.g., SVM, Decision Trees).
 ‚Ä¢ Wrote code with detailed comments to improve clarity and reusability.
 ‚Ä¢ Analyzed and interpreted the results of each algorithm to identify the best-performing model.

[WAAD]
 ‚Ä¢ Finalized the data cleaning and transformation before model training.
 ‚Ä¢ Encoded categorical features if needed and standardized numerical features using StandardScaler.
 ‚Ä¢ Ensured the dataset was free of inconsistent entries, empty cells, or formatting issues.
 ‚Ä¢ Helped prepare a clean dataset that was directly used to train the models in this phase.

[HUTOON]
 I was responsible for testing supervised learning algorithms and comparing their results to help determine which models worked best.

 [Abrar]
 Implemented one of the supervised learning algorithms used in the project.

 [LAYAN]
‚Ä¢ Implemented the SVM (Support Vector Machine) algorithm using the training dataset and fine-tuned hyperparameters to improve model accuracy.
‚Ä¢ Visualized performance using the confusion matrix and evaluated results using metrics like accuracy and classification report.
‚Ä¢ Participated in modifying the SVM model to reduce overfitting by adjusting C, kernel, gamma, and max_iter.
‚Ä¢ Collaborated with the team to compare SVM with Random Forest and contributed to the decision-making on which algorithm performs better.

üîπ Phase 3: Unsupervised Learning

[BODOR]
 ‚Ä¢ Evaluated clustering performance using metrics such as:
 ‚Ä¢ Silhouette Coefficient
 ‚Ä¢ Total Within-Cluster Sum of Squares (WCSS)
 ‚Ä¢ BCubed Precision and Recall
 ‚Ä¢ Created visualizations (e.g., cluster plots) to demonstrate group separations.
 ‚Ä¢ Discussed how clustering could enhance recommendations and its limitations.

[WAAD]
 ‚Ä¢ Created the Jupyter Notebook for this phase and implemented DBSCAN, K-Means, and Agglomerative Clustering (HAC).
 ‚Ä¢ Used StandardScaler for scaling features and applied PCA for visualizing clustering results in 2D.
 ‚Ä¢ Produced multiple visualizations (e.g., scatter plots, dendrogram, elbow method).
 ‚Ä¢ Explained the outcomes and compared the clustering algorithms based on performance metrics.

[HUTOON]
I handled the visualizations for clustering algorithms. I created clear visual representations for K-Means, DBSCAN, and HAC using PCA to show how the data was grouped.

[Abrar]

Designed and implemented the unsupervised learning algorithm (clustering) for the project.
‚Ä¢ Preprocessed the dataset for clustering (e.g., removing class labels).

[LAYAN]
‚Ä¢ Worked on enhancing the DBSCAN algorithm by adjusting the eps and min_samples parameters to improve clustering performance.
‚Ä¢ Ran several experiments to reduce the number of clusters while keeping a reasonable Silhouette Score.
‚Ä¢ Tested different configurations and helped the team decide on the most logical DBSCAN setup for interpretation.
‚Ä¢ Supported in evaluating and comparing results from K-Means, HAC, and DBSCAN, and summarized findings to support model selection.

üîπ Phase 4: Integrating Generative AI

[BODOR]

Handles the user interface where the user enters symptoms.
Prepares the symptoms as input data for the prediction model.

[WAAD]

Documents the entire Phase 4 work.
Compares the outputs of both response templates and justifies which one is more effective.

[HUTOON]

Created the Generative_AI folder.
Added the gpt_response_template.md file with two response formats (Formal and Friendly).
Wrote the gpt_response.py script to connect the predicted disease to GPT using the OpenAI API.
Prepared the code to use the API key from GitHub Secrets.
Added a placeholder disease name until the model is connected.

[Abrar]

Uses the trained machine learning model to predict the disease from the user input.
Passes the predicted disease name to the GPT response script.

[LAYAN]

Adds the API key to GitHub Secrets and completes the integration with GPT.